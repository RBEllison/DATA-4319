---
title: "Logistic Regression Algorithm"
subtitle : "Using Breast Cancer Data"
author: 'Author : Rose Ellison'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(magrittr)
library(tidyverse)
library(dplyr)
library(sqldf)
library(caTools)
library(ggplot2)
library(gridExtra)
```

![]("../../../_resources/images/cancer.jpg")

\newpage

## The Intuition
Logistic regression is similiar to simple and multiple regression. The difference is it is a classifier of binary data instead of predicting continuous variables. Furthermore, the logistic regression model uses probability to predict the dependent variable.<br>

$$\hat y_i = \sigma (w^Tx_i + b)$$

$$L_{ce}(\hat y_i, y_i) = -[y_i\  log \ \hat y_i + (1 - y_i) \ log(1 - \hat y_i)]$$

$$Cost(w, b) = \frac{1}{N} \sum L_{ce}(\hat y_i, y_i)$$
$$gradient = \frac{\partial l_{ce}(w,b)}{\partial w_i}= [\sigma(w^Tx_i + b) - y] x_i $$
$$ Bias = [\sigma(w^Tx_i + b) - y] $$

 

### Preprocessing
```{r}
# Read the data
data = read.csv('../../../_resources/data/cancer.csv')
data$diagnosis <- ifelse(data$diagnosis == 'M', 1, 0)
data <- data[ , c(2, 30)]
head(data)
```

\newpage

### Plot
```{r}
plot(data$concave.points_worst, data$diagnosis)
title("Breast Cancer : Diagnosis vs Concave Points")
```


## Implementation Code


### Step One : 
Split the data into training and test set.

```{r}
# Splitting test and training sets
split <- sample.split(data[ , 1], SplitRatio = 0.8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
```




### Step Two : 
Convert x and y to matrix.
```{r}
x_training <- as.matrix(na.omit(data$concave.points_worst))
y_training <- as.matrix(na.omit(data$diagnosis))
```




### Step Three : 
Define sigmoid, cross entropy and average cost function.
```{r}
sigmoid <- function(x)
{
  return(1 / (1 + exp(-x))) 
}


cost <- function(theta, X, y){
  m <- length(y) # number of training examples

  h <- sigmoid(X%*%theta)
  J <- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m
  J
}

#gradient function
grad <- function(theta, X, y){
  m <- length(y) 

  h <- sigmoid(X%*%theta)
  grad <- (t(X)%*%(h - y))/m
  grad
}
```

### Step Four
```{r}
logisticReg <- function(X, y){
  #add bias term and convert to matrix
  # X <- mutate(temp[, -1], bias =1)
  X <- as.matrix(X[,c(ncol(X), 1:(ncol(X)-1))])
  y <- as.matrix(temp[, 1])
  #initialize theta
  theta <- matrix(rep(0, ncol(X)), nrow = ncol(X))
  #use the optim function to perform gradient descent
  costOpti <- optim(matrix(rep(0, 4), nrow = 4), cost, grad(theta, x, y), X=X, y=y)
  #return coefficients
  return(costOpti$par)
}

logisticReg(x_training, y_training)
```



## Results
```{r}

```
fbjds hjdsk hdjksa ak




