---
title: "K-Nearest Neighbors Algorithm"
subtitle : "Using Social Network Ad Data"
author: 'Author : Rose Ellison'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(magrittr)
library(tidyverse)
library(dplyr)
library(sqldf)
library(caTools)
library(ggplot2)
library(gridExtra)
```

![]("../../../_resources/images/ads.jpg")

\newpage

## The Intuition
K-Nearest Neigbors(KNN) is a type of supervised machine learning algorithm which can be used for both classification and regression problems. It is typically used for predictive problems.<br>

**Step One: ** Split the data into test and training sets

**Step Two : ** Choose the value K, which can be any integer.
 
**Step Three : ** For each point in the test data do the following
 
   - **3A : ** Calculate the distance beteen test data and each row of training data. We will use the Euclidean distance method.
  
$$d(p, q) = \sqrt{\sum{(q_i - p_i)^2}}$$

  - **3B : ** Sort the distance values in ascending order
    
  - **3C : ** Choose the top K rows from the sorted array
    
  - **3D : ** Assign a class to the test point based on most frequent class of these rows.
 

## Exploration
Exploration process includes preprocessing and visualizing plots. This algorithm will be implemented on social network ads data. There are three columns; purchased, salary, and age. We will be trying to predict if future customers will purchase a social network ad.

### Preprocessing
```{r}
# Read the data
data = read.csv('../../../_resources/data/Social_Network_Ads.csv')
data <- data[, 3:5]
# Feature Scaling
data[-3] = scale(data[-3])
```

\newpage

### Plot
```{r}
ggplot(data,aes(x = data[, 1], 
                y = data[, 2], 
                color = factor(data[ , 3]))) + 
  geom_point() +
  labs(x = "Salary", 
       y = "Age") + 
  ggtitle("Purchased : Salary vs Age")
```

From the plot, we can see this data is linearly seperable. The KNN algorithm classifier should give us the line of best fit that best seperates this data.

## Implementation

### Step One : 
Split the data into test and training sets.

```{r}
# Splitting test and training sets
split <- sample.split(data[ , 3], SplitRatio = 0.8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
```

### Step Two : 
Choose the value K, which can be any integer.
```{r}
k <- 5
```

### Step Three : 
For each point in the test data do the following,
1.) Calculate the distance beteen test data and each row of training data. We will use the Euclidean distance method. 2.) Sort the distance values in ascending order. 3.) Choose the top K rows from the sorted array. 4.) Assign a class to the test point based on most frequent class of these rows.

#### 3A.) 
```{r}
# Euc distance per point
get_euc_dist <- function(p, q)
{
  # Euclidean Distance calculation
  return(sqrt(sum(p - q) ^ 2))
}

```

#### 4A.)
```{r}
knn <- function(data, k)
{

  for (i in 1: nrow(data))
  {
    # Loop through all points 
    for (j in 1:nrow(data))
    {
      # Save distances in list
      data$Distance[i] <- get_euc_dist(data[j, 1], data[j, 2])
      
    }
    
    neighbors <- data %>% 
                  arrange(Distance) %>% 
                  head(k)
    
    zero_count <- sum(neighbors$Purchased == 0)
    one_count <- sum(neighbors$Purchased == 1)
    
    
    if(zero_count > one_count)
    {
      data$y_pred[i] <- 0
    } else
    {
      data$y_pred[i] <- 1
    }
    
  }
  return(data$y_pred)
}

training$y_pred <- knn(training, k)
```
## Results
```{r}
training %>% 
  select(y_pred, Purchased)
```
Although the algorithm is correct, we can see the age and salary are not good predictors of if the customer purchased from social media ads. This is most likely due to multicollinearity between age and salary. We would need to drop one of the predictors and find a new new one.




